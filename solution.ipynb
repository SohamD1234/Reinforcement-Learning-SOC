{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Dynamic Programming - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pulp\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, filename: str, quiet=False) -> None:\n",
    "        self.name = filename.split('.')[0]\n",
    "        stt = time.time()\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                ls = line.split()\n",
    "                if ls[0] == 'states':\n",
    "                    self.N_s = int(ls[1])\n",
    "                elif ls[0] == 'actions':\n",
    "                    self.N_a = int(ls[1])\n",
    "                    self.probs = [[[{} for _ in range(self.N_s)] for _ in range(self.N_a)] for _ in range(self.N_s)]\n",
    "                elif ls[0] == 'tran':\n",
    "                    self.probs[int(ls[1])][int(ls[2])][int(ls[3])][np.float64(ls[4])] = np.float64(ls[5])\n",
    "                else:\n",
    "                    assert(ls[0] == 'gamma')\n",
    "                    self.gamma = np.float64(ls[1])\n",
    "        if not quiet:\n",
    "            print(f'Time taken to generate MDP from {filename}: {time.time() - stt} s')\n",
    "\n",
    "\n",
    "\n",
    "class PolicyIterationSolver:\n",
    "    '''Hyperparameters of this implementation of Policy Iteration'''\n",
    "    EVAL_THR = 1e-5\n",
    "    IMP_THR = 1e-7 # Required to avoid switching between multiple optimal policies infinitely often\n",
    "\n",
    "    def __init__(self, MDP: MDP) -> None:\n",
    "        self._MDP = MDP\n",
    "        self._policy = np.random.choice(self._MDP.N_a, self._MDP.N_s)\n",
    "        self._value = np.zeros(self._MDP.N_s, np.float64)\n",
    "\n",
    "    def _policy_evaluation(self) -> np.float64:\n",
    "        old_policy = copy.copy(self._value)\n",
    "        while True:\n",
    "            mdiff = 0.0\n",
    "            for s in range(self._MDP.N_s):\n",
    "                prev_value = self._value[s]\n",
    "                '''An in-place implementation'''\n",
    "                self._value[s] = np.sum([np.sum([self._MDP.probs[s][self._policy[s]][ss][r]*(r + self._MDP.gamma*self._value[ss]) for r in self._MDP.probs[s][self._policy[s]][ss]]) for ss in range(self._MDP.N_s)])\n",
    "                mdiff = np.maximum(np.float64(mdiff), np.abs(self._value[s] - prev_value))\n",
    "            if mdiff < PolicyIterationSolver.EVAL_THR:\n",
    "                return np.max(np.abs(self._value - old_policy)) # type: ignore\n",
    "    \n",
    "    def _policy_improvement(self) -> None:\n",
    "        for s in range(self._MDP.N_s):\n",
    "            self._policy[s] = np.argmax([np.sum([np.sum([self._MDP.probs[s][a][ss][r]*(r + self._MDP.gamma*self._value[ss]) for r in self._MDP.probs[s][a][ss]]) for ss in range(self._MDP.N_s)]) for a in range(self._MDP.N_a)])\n",
    "    \n",
    "    '''Implements policy iteration and returns optimal value, optimal policy'''\n",
    "    def solve(self, quiet=False) -> tuple:\n",
    "        stt = time.time()\n",
    "        while self._policy_evaluation() >= PolicyIterationSolver.IMP_THR:\n",
    "            self._policy_improvement()\n",
    "        if not quiet:\n",
    "            print(f'Time taken for Policy Iteration on {self._MDP.name}: {time.time() - stt} s')\n",
    "        return self._value, self._policy\n",
    "\n",
    "\n",
    "\n",
    "class ValueIterationSolver:\n",
    "    '''Hyperparameters of this implementation of Value Iteration'''\n",
    "    THR = 1e-5\n",
    "\n",
    "    def __init__(self, MDP: MDP) -> None:\n",
    "        self._MDP = MDP\n",
    "        self._value = np.zeros(self._MDP.N_s, np.float64)\n",
    "\n",
    "    '''Implements value iteration and returns optimal value, optimal policy'''\n",
    "    def solve(self, quiet=False) -> tuple:\n",
    "        stt = time.time()\n",
    "        while True:\n",
    "            mdiff = 0.0\n",
    "            for s in range(self._MDP.N_s):\n",
    "                prev_value = self._value[s]\n",
    "                '''An in-place implementation'''\n",
    "                self._value[s] = np.max([np.sum([np.sum([self._MDP.probs[s][a][ss][r]*(r + self._MDP.gamma*self._value[ss]) for r in self._MDP.probs[s][a][ss]]) for ss in range(self._MDP.N_s)]) for a in range(self._MDP.N_a)])\n",
    "                mdiff = np.maximum(mdiff, np.abs(self._value[s] - prev_value))\n",
    "                if mdiff < ValueIterationSolver.THR:\n",
    "                    policy = np.array([np.argmax([np.sum([np.sum([self._MDP.probs[s][a][ss][r]*(r + self._MDP.gamma*self._value[ss]) for r in self._MDP.probs[s][a][ss]]) for ss in range(self._MDP.N_s)]) for a in range(self._MDP.N_a)]) for s in range(self._MDP.N_s)])\n",
    "                    if not quiet:\n",
    "                        print(f'Time taken for Value Iteration on {self._MDP.name}: {time.time() - stt} s')\n",
    "                    return self._value, policy\n",
    "\n",
    "\n",
    "\n",
    "class LinearProgrammingSolver:\n",
    "    def __init__(self, MDP: MDP) -> None:\n",
    "        self._MDP = MDP\n",
    "        self._model = pulp.LpProblem(\"MDP\", pulp.LpMinimize)\n",
    "        self._vals = [pulp.LpVariable(f'{s}') for s in range(self._MDP.N_s)]\n",
    "    \n",
    "    '''Implements Linear Programming and returns the optimal value and optimal policy'''\n",
    "    def solve(self, quiet=False):\n",
    "        stt = time.time()\n",
    "        self._model += sum(self._vals) # Objective function\n",
    "        for a in range(self._MDP.N_a):\n",
    "            for s in range(self._MDP.N_s):\n",
    "                coeffs = np.zeros(self._MDP.N_s, np.float64)\n",
    "                coeffs[s] = 1\n",
    "                const = 0.0\n",
    "                for ss in range(self._MDP.N_s):\n",
    "                    for r in self._MDP.probs[s][a][ss]:\n",
    "                        coeffs[ss] -= self._MDP.probs[s][a][ss][r]*self._MDP.gamma\n",
    "                        const += r*self._MDP.probs[s][a][ss][r]\n",
    "                self._model += sum([coeffs[s]*self._vals[s] for s in range(self._MDP.N_s)]) >= const # Constraints\n",
    "        self._model.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "        assert(self._model.status == 1)\n",
    "        values = np.zeros(self._MDP.N_s, np.float64)\n",
    "        for v in self._model.variables():\n",
    "            values[int(v.name)] = v.varValue\n",
    "        policy = np.array([np.argmax([np.sum([np.sum([self._MDP.probs[s][a][ss][r]*(r + self._MDP.gamma*values[ss]) for r in self._MDP.probs[s][a][ss]]) for ss in range(self._MDP.N_s)]) for a in range(self._MDP.N_a)]) for s in range(self._MDP.N_s)])\n",
    "        if not quiet:\n",
    "            print(f'Time taken for solving the Linear Program of {self._MDP.name}: {time.time() - stt} s')\n",
    "        return values, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_to_file(filename: str, solution: tuple) -> None:\n",
    "    assert(len(solution) == 2 and len(solution[0]) == len(solution[1]))\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(len(solution[0])):\n",
    "            f.write(f'{round(solution[0][i], 6)} {solution[1][i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to generate MDP from MDPs/MDPs/mdp-2-2.txt: 0.0007128715515136719 s\n",
      "Time taken for Policy Iteration on MDPs/MDPs/mdp-2-2: 0.014780759811401367 s\n",
      "Time taken for Value Iteration on MDPs/MDPs/mdp-2-2: 0.012240409851074219 s\n",
      "Time taken for solving the Linear Program of MDPs/MDPs/mdp-2-2: 0.003809690475463867 s\n",
      "Time taken to generate MDP from MDPs/MDPs/mdp-40-50.txt: 0.23939967155456543 s\n",
      "Time taken for Policy Iteration on MDPs/MDPs/mdp-40-50: 2.2574880123138428 s\n",
      "Time taken for Value Iteration on MDPs/MDPs/mdp-40-50: 4.316442012786865 s\n",
      "Time taken for solving the Linear Program of MDPs/MDPs/mdp-40-50: 1.6045176982879639 s\n",
      "Time taken to generate MDP from MDPs/MDPs/mdp-3-5.txt: 0.00027942657470703125 s\n",
      "Time taken for Policy Iteration on MDPs/MDPs/mdp-3-5: 0.0009758472442626953 s\n",
      "Time taken for Value Iteration on MDPs/MDPs/mdp-3-5: 0.0011119842529296875 s\n",
      "Time taken for solving the Linear Program of MDPs/MDPs/mdp-3-5: 0.003379344940185547 s\n",
      "Time taken to generate MDP from MDPs/MDPs/mdp-10-5.txt: 0.00020933151245117188 s\n",
      "Time taken for Policy Iteration on MDPs/MDPs/mdp-10-5: 0.05849456787109375 s\n",
      "Time taken for Value Iteration on MDPs/MDPs/mdp-10-5: 0.04980897903442383 s\n",
      "Time taken for solving the Linear Program of MDPs/MDPs/mdp-10-5: 0.008493185043334961 s\n",
      "Time taken to generate MDP from MDPs/MDPs/mdp-10-10.txt: 0.002176523208618164 s\n",
      "Time taken for Policy Iteration on MDPs/MDPs/mdp-10-10: 0.0891733169555664 s\n",
      "Time taken for Value Iteration on MDPs/MDPs/mdp-10-10: 0.11897611618041992 s\n",
      "Time taken for solving the Linear Program of MDPs/MDPs/mdp-10-10: 0.020148515701293945 s\n",
      "Time taken to generate MDP from MDPs/MDPs/mdp-20-30.txt: 0.028458833694458008 s\n",
      "Time taken for Policy Iteration on MDPs/MDPs/mdp-20-30: 0.3545377254486084 s\n",
      "Time taken for Value Iteration on MDPs/MDPs/mdp-20-30: 0.5974388122558594 s\n",
      "Time taken for solving the Linear Program of MDPs/MDPs/mdp-20-30: 0.1792759895324707 s\n",
      "Time taken to generate MDP from MDPs/MDPs/mdp-100-100.txt: 2.390565872192383 s\n",
      "Time taken for Policy Iteration on MDPs/MDPs/mdp-100-100: 187.06220364570618 s\n",
      "Time taken for Value Iteration on MDPs/MDPs/mdp-100-100: 367.2891161441803 s\n",
      "Time taken for solving the Linear Program of MDPs/MDPs/mdp-100-100: 27.249027729034424 s\n"
     ]
    }
   ],
   "source": [
    "for mdpfile in os.listdir('MDPs/MDPs/'):\n",
    "    mdp = MDP(f'MDPs/MDPs/{mdpfile}')\n",
    "    print_to_file(f'MDPs/PI_solutions/sol-{mdpfile}', PolicyIterationSolver(mdp).solve())\n",
    "    print_to_file(f'MDPs/VI_solutions/sol-{mdpfile}', ValueIterationSolver(mdp).solve())\n",
    "    print_to_file(f'MDPs/LP_solutions/sol-{mdpfile}', LinearProgrammingSolver(mdp).solve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
